import streamlit as st
from core.data_fetcher import get_platform_data
from core.crawl.dispatcher import run_batch
from scrapers.logger import log_info
import os
import json


def render_amazon_crawl_tool():
    """æ¸²æŸ“Amazoné‡‡é›†å·¥å…·é¡µé¢"""
    def render_amazon_crawl_tool():
        """æ¸²æŸ“Amazoné‡‡é›†å·¥å…·é¡µé¢"""
        st.header("ğŸ›’ Amazoné‡‡é›†å·¥å…·ï¼ˆå…¨é‡å¢å¼ºç‰ˆ + è‡ªè¿­ä»£æ§åˆ¶å°ï¼‰")

        # åˆ›å»ºä¸¤ä¸ªä¸»æ ‡ç­¾é¡µ
        main_tab1, main_tab2 = st.tabs(["æ•°æ®é‡‡é›†", "çˆ¬è™«è‡ªæˆ‘è¿­ä»£æ§åˆ¶å°"])

    with main_tab1:
        st.markdown("### ğŸ“Š Amazonæ•°æ®é‡‡é›†")
    
        mode = st.radio("æ¨¡å¼é€‰æ‹©", ["å•é¡µé‡‡é›†", "æ‰¹é‡URLé‡‡é›†", "APIæ¥å£æ¨¡å¼"], horizontal=True)

        storage_mode = st.selectbox("å­˜å‚¨æ¨¡å¼", ["local", "mongo", "mysql", "cloud"], index=0)
        deep_detail = st.checkbox("é‡‡é›†è¯¦æƒ…é¡µï¼ˆåŒ…å«è¯„è®ºã€è§„æ ¼ç­‰ï¼‰", value=True)
        max_items = st.slider("å•é¡µæœ€å¤§å•†å“æ•°", 10, 200, 50, 10)
    
        # APIå¯†é’¥é…ç½®ï¼ˆç”¨äºAPIæ¨¡å¼ï¼‰
        if mode == "APIæ¥å£æ¨¡å¼":
            st.info("ğŸ’¡ å¦‚æœçˆ¬è™«çˆ¬å–å¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨APIæ¥å£è·å–æ•°æ®")
            api_key = st.text_input("Amazon Product APIå¯†é’¥", type="password", 
                                   placeholder="å¯é€‰ï¼šè¾“å…¥ç¬¬ä¸‰æ–¹Amazonæ•°æ®APIå¯†é’¥")
            st.caption("æ”¯æŒçš„API: RapidAPI Amazon Product Data, Rainforest APIç­‰")

        if mode == "å•é¡µé‡‡é›†":
            pattern = st.radio("é¡µé¢ç±»å‹", ["Bestseller", "å…³é”®è¯æœç´¢", "åˆ†ç±»URL"], horizontal=True)
            keyword = ""
            category_url = ""
            if pattern == "å…³é”®è¯æœç´¢":
                keyword = st.text_input("å…³é”®è¯", value="laptop")
            elif pattern == "åˆ†ç±»URL":
                category_url = st.text_input("åˆ†ç±»URL", value="https://www.amazon.com/bestsellers")

            if st.button("å¼€å§‹å•é¡µé‡‡é›† ğŸš€", type="primary"):
                with st.spinner("é‡‡é›†ä¸­..."):
                    try:
                        data = get_platform_data(
                            platform_name="Amazon",
                            keyword=keyword,
                            category_url=category_url,
                            max_items=max_items,
                            deep_detail=deep_detail
                        )
                    
                        if data:
                            st.success(f"å®Œæˆï¼Œé‡‡é›† {len(data)} æ¡æ•°æ®")
                        
                            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                            st.markdown("#### æ•°æ®é¢„è§ˆï¼ˆå‰5æ¡ï¼‰")
                            for idx, item in enumerate(data[:5], 1):
                                with st.expander(f"ğŸ“¦ {idx}. {item.get('title', 'Unknown')[:100]}"):
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.write("**ASIN:**", item.get('asin', 'N/A'))
                                        st.write("**ä»·æ ¼:**", item.get('price', item.get('current_price', 'N/A')))
                                        st.write("**å“ç‰Œ:**", item.get('brand', 'N/A'))
                                        st.write("**è¯„åˆ†:**", item.get('rating', item.get('average_rating', 'N/A')))
                                    with col2:
                                        st.write("**è¯„è®ºæ•°:**", item.get('review_count', 'N/A'))
                                        st.write("**BSR:**", item.get('bsr_ranking', 'N/A')[:50] if item.get('bsr_ranking') else 'N/A')
                                        st.write("**FBA:**", "âœ…" if item.get('is_fba') else "âŒ")
                                        st.write("**åº“å­˜:**", item.get('stock_status', 'N/A')[:30] if item.get('stock_status') else 'N/A')
                        
                            # æ˜¾ç¤ºå®Œæ•´JSON
                            with st.expander("ğŸ“„ æŸ¥çœ‹å®Œæ•´JSONæ•°æ®"):
                                st.json(data)
                        else:
                            st.error("æœªé‡‡é›†åˆ°æ•°æ®ã€‚è¯·å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š")
                            st.markdown("""
                            1. æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®
                            2. åˆ‡æ¢åˆ°"APIæ¥å£æ¨¡å¼"ä½¿ç”¨APIè·å–æ•°æ®
                            3. æŸ¥çœ‹çˆ¬è™«è‡ªæˆ‘è¿­ä»£æ§åˆ¶å°è¿›è¡Œè°ƒä¼˜
                            4. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: scraper.log
                            """)
                    except Exception as e:
                        st.error(f"é‡‡é›†å¤±è´¥: {e}")
                    
                        # æä¾›APIæ¥å£é€‰é¡¹
                        if st.button("ğŸ”„ åˆ‡æ¢åˆ°APIæ¥å£æ¨¡å¼"):
                            st.info("åˆ‡æ¢åˆ°'APIæ¥å£æ¨¡å¼'æ ‡ç­¾é¡µï¼Œä½¿ç”¨ç¬¬ä¸‰æ–¹APIè·å–æ•°æ®")

        elif mode == "æ‰¹é‡URLé‡‡é›†":
            st.write("æ‰¹é‡æ¨¡å¼ï¼šè¾“å…¥å¤šä¸ª URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
            urls_text = st.text_area("URL åˆ—è¡¨", value="https://www.amazon.com/bestsellers\nhttps://www.amazon.com/s?k=usb+hub", height=150)
        
            if st.button("å¼€å§‹æ‰¹é‡é‡‡é›† ğŸ§©", type="primary"):
                urls = [u.strip() for u in urls_text.splitlines() if u.strip()]
                if not urls:
                    st.error("è¯·æä¾›è‡³å°‘ä¸€ä¸ª URLã€‚")
                else:
                    st.info(f"å…± {len(urls)} ä¸ªä»»åŠ¡ï¼Œå¼€å§‹è°ƒåº¦...")
                    with st.spinner("æ‰¹é‡é‡‡é›†ä¸­..."):
                        try:
                            run_batch(urls, storage_mode=storage_mode)
                            st.success("æ‰¹é‡ä»»åŠ¡å·²å®Œæˆï¼ˆæŸ¥çœ‹ data/ æˆ–æ•°æ®åº“ä¸­ç»“æœï¼‰ã€‚")
                        except Exception as e:
                            st.error(f"æ‰¹é‡é‡‡é›†å¤±è´¥: {e}")
    
        elif mode == "APIæ¥å£æ¨¡å¼":
            st.markdown("### ğŸ”Œ ä½¿ç”¨APIæ¥å£è·å–Amazonæ•°æ®")
            st.info("æ­¤æ¨¡å¼é€šè¿‡ç¬¬ä¸‰æ–¹APIè·å–æ•°æ®ï¼Œé¿å…çˆ¬è™«è¢«å°ç¦")
        
            api_endpoint = st.text_input("APIç«¯ç‚¹URL", placeholder="https://api.example.com/amazon/products")
        
            col1, col2 = st.columns(2)
            with col1:
                search_term = st.text_input("æœç´¢å…³é”®è¯", value="laptop")
            with col2:
                api_max_results = st.number_input("ç»“æœæ•°é‡", min_value=1, max_value=100, value=50)
        
            if st.button("ğŸ“¡ é€šè¿‡APIè·å–æ•°æ®", type="primary"):
                st.warning("âš ï¸ æ­¤åŠŸèƒ½éœ€è¦æœ‰æ•ˆçš„ç¬¬ä¸‰æ–¹APIå¯†é’¥")
                st.info("è¯·è”ç³»APIæä¾›å•†è·å–å¯†é’¥ï¼Œå¦‚: RapidAPI, Rainforest API, ScraperAPIç­‰")
            
                # ç¤ºä¾‹ï¼šæ˜¾ç¤ºå¦‚ä½•ä½¿ç”¨API
                st.markdown("""
                **APIä½¿ç”¨ç¤ºä¾‹ä»£ç :**
                ```python
                import requests
            
                url = "https://rapidapi.com/api/amazon-product"
                headers = {
                    "X-RapidAPI-Key": "your_api_key",
                    "X-RapidAPI-Host": "amazon-product.p.rapidapi.com"
                }
                params = {
                    "query": "laptop",
                    "max_results": 50
                }
            
                response = requests.get(url, headers=headers, params=params)
                data = response.json()
                ```
                """)

        st.divider()
        st.markdown("**ğŸ’¡ æç¤ºï¼š**")
        st.markdown("- æŸ¥çœ‹æ ¹ç›®å½• scraper.log è·å–è¯¦ç»†æ—¥å¿—")
        st.markdown("- å¦‚æœçˆ¬å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨'çˆ¬è™«è‡ªæˆ‘è¿­ä»£æ§åˆ¶å°'ä¼˜åŒ–çˆ¬è™«")
        st.markdown("- æˆ–åˆ‡æ¢åˆ°'APIæ¥å£æ¨¡å¼'ä½¿ç”¨ç¬¬ä¸‰æ–¹API")

    with main_tab2:
        st.markdown("### ğŸ§¬ çˆ¬è™«è‡ªæˆ‘è¿­ä»£æ§åˆ¶å°ï¼ˆé›†æˆä¼˜åŒ–ï¼‰")
        st.info("æ­¤æ§åˆ¶å°ç”¨äºç›‘æ§å’Œä¼˜åŒ–Amazonçˆ¬è™«æ€§èƒ½")
    
        # å¯¼å…¥è¿­ä»£å¼•æ“
        try:
            from core.auto_crawler_iter.iteration_engine import CrawlerIterationEngine
            from core.auto_crawler_iter.metrics_collector import MetricsCollector
        
            engine = CrawlerIterationEngine()
            collector = MetricsCollector()
        
            # æ˜¾ç¤ºå½“å‰æŒ‡æ ‡
            col1, col2 = st.columns(2)
        
            with col1:
                st.markdown("#### ğŸ“Š å½“å‰çˆ¬è™«æŒ‡æ ‡")
                if st.button("ğŸ”„ åˆ·æ–°æŒ‡æ ‡", key="refresh_metrics"):
                    metrics = collector.collect()
                
                    st.metric("æŠ“å–å•†å“æ€»æ•°", metrics.get('items_total', 0))
                    st.metric("é›¶ç»“æœé¡µé¢æ•°", metrics.get('pages_zero', 0))
                    st.metric("é”™è¯¯æ¬¡æ•°", metrics.get('errors_total', 0))
                    st.metric("éªŒè¯ç å‘½ä¸­æ¬¡æ•°", metrics.get('captcha_hits', 0))
                    st.metric("å¹³å‡é¡µé¢åŠ è½½æ—¶é—´", f"{metrics.get('avg_list_time', 0):.2f}ç§’")
                
                    # è®¡ç®—æˆåŠŸç‡
                    total_pages = metrics.get('items_total', 0) + metrics.get('pages_zero', 0)
                    if total_pages > 0:
                        success_rate = (metrics.get('items_total', 0) / total_pages) * 100
                        st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
        
            with col2:
                st.markdown("#### ğŸ”§ ä¼˜åŒ–æ§åˆ¶")
            
                if st.button("â–¶ï¸ è¿è¡Œä¸€è½®è¿­ä»£ä¼˜åŒ–", key="run_iteration", type="primary"):
                    with st.spinner("æ­£åœ¨è¿è¡Œè¿­ä»£ä¼˜åŒ–..."):
                        result = engine.run_once()
                        st.success("âœ… è¿­ä»£ä¼˜åŒ–å®Œæˆ")
                        st.json(result)
            
                st.markdown("---")
            
                # è‡ªåŠ¨ä¼˜åŒ–å¼€å…³
                auto_optimize = st.checkbox("å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–ï¼ˆå®šæ—¶è¿è¡Œï¼‰", value=False)
                if auto_optimize:
                    st.info("è‡ªåŠ¨ä¼˜åŒ–å°†åœ¨åå°è¿è¡Œï¼Œæ¯2å°æ—¶æ£€æŸ¥ä¸€æ¬¡")
                    st.caption("é€šè¿‡scheduler.pyé…ç½®è‡ªåŠ¨ä¼˜åŒ–é—´éš”")
        
            st.divider()
        
            # æ˜¾ç¤ºè¡¥ä¸åˆ—è¡¨
            st.markdown("#### ğŸ©¹ å€™é€‰è¡¥ä¸åˆ—è¡¨")
            patch_dir = engine.cfg.get("patch_output_dir", "sandbox/patches")
        
            patches = []
            if os.path.isdir(patch_dir):
                patches = [f for f in os.listdir(patch_dir) if f.endswith(".patch")]
        
            if not patches:
                st.info("æš‚æ— è¡¥ä¸å€™é€‰ã€‚ç‚¹å‡»ä¸Šæ–¹'è¿è¡Œä¸€è½®è¿­ä»£ä¼˜åŒ–'ç”Ÿæˆè¡¥ä¸ã€‚")
            else:
                st.success(f"æ‰¾åˆ° {len(patches)} ä¸ªå€™é€‰è¡¥ä¸")
            
                for idx, p in enumerate(patches, 1):
                    tag = p.replace(".patch", "")
                
                    with st.expander(f"ğŸ©¹ è¡¥ä¸ {idx}: {p}"):
                        # æ˜¾ç¤ºè¡¥ä¸å†…å®¹
                        try:
                            with open(os.path.join(patch_dir, p), "r", encoding="utf-8") as f:
                                patch_content = f.read()
                            st.code(patch_content, language="diff")
                        except Exception as e:
                            st.error(f"è¯»å–è¡¥ä¸å¤±è´¥: {e}")
                    
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button(f"âœ… åº”ç”¨è¡¥ä¸", key=f"apply_{tag}"):
                                try:
                                    res = engine.apply_patch(tag)
                                    st.success(f"è¡¥ä¸å·²åº”ç”¨: {res}")
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"åº”ç”¨è¡¥ä¸å¤±è´¥: {e}")
                    
                        with col2:
                            if st.button(f"ğŸ—‘ï¸ åˆ é™¤è¡¥ä¸", key=f"delete_{tag}"):
                                try:
                                    os.remove(os.path.join(patch_dir, p))
                                    st.success("è¡¥ä¸å·²åˆ é™¤")
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
        
            st.divider()
            st.caption("ğŸ’¡ æç¤ºï¼šè¡¥ä¸ä¼šä¿®æ”¹ scrapers/amazon_scraper.py ä¸­çš„é€‰æ‹©å™¨å’Œå‚æ•°ï¼Œæå‡çˆ¬å–æˆåŠŸç‡")
        
        except ImportError as e:
            st.error(f"æ— æ³•åŠ è½½è¿­ä»£å¼•æ“: {e}")
            st.info("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
        except Exception as e:
            st.error(f"è¿­ä»£æ§åˆ¶å°åŠ è½½å¤±è´¥: {e}")
            import traceback
            with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…"):
                st.code(traceback.format_exc())

# Auto-execute if run directly
if __name__ == "__main__":
    render_amazon_crawl_tool()