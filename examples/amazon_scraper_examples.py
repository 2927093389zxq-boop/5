#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Amazonçˆ¬è™«ä½¿ç”¨ç¤ºä¾‹
Amazon Scraper Usage Examples

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Amazonçˆ¬è™«è¿›è¡Œæ•°æ®é‡‡é›†
Demonstrates how to use Amazon scraper for data collection
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ / Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from scrapers.amazon_scraper import AmazonScraper, scrape_amazon
import json


def example1_basic_scraping():
    """
    ç¤ºä¾‹1: åŸºç¡€é‡‡é›†
    Example 1: Basic scraping
    """
    print("=" * 60)
    print("ç¤ºä¾‹1: åŸºç¡€é‡‡é›† / Example 1: Basic Scraping")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ / Create scraper instance
    scraper = AmazonScraper()
    
    # é‡‡é›†æœç´¢ç»“æœé¡µ / Scrape search results page
    url = "https://www.amazon.com/s?k=laptop"
    print(f"\næ­£åœ¨é‡‡é›† / Scraping: {url}")
    
    products = scraper.scrape_list_page(url, max_items=10)
    
    print(f"\nâœ… é‡‡é›†å®Œæˆï¼Œå…± {len(products)} ä¸ªå•†å“ / Completed, {len(products)} products")
    
    if products:
        print("\nå‰3ä¸ªå•†å“é¢„è§ˆ / First 3 products preview:")
        for i, product in enumerate(products[:3], 1):
            print(f"\nå•†å“ {i} / Product {i}:")
            print(f"  æ ‡é¢˜ / Title: {product.get('title', 'N/A')[:50]}...")
            print(f"  ä»·æ ¼ / Price: {product.get('price', 'N/A')}")
            print(f"  è¯„åˆ† / Rating: {product.get('rating', 'N/A')}")
            print(f"  ASIN: {product.get('asin', 'N/A')}")


def example2_detail_scraping():
    """
    ç¤ºä¾‹2: è¯¦æƒ…é¡µé‡‡é›†
    Example 2: Detail page scraping
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹2: è¯¦æƒ…é¡µé‡‡é›† / Example 2: Detail Page Scraping")
    print("=" * 60)
    
    scraper = AmazonScraper()
    
    # ä½¿ç”¨ç¤ºä¾‹ASIN / Use example ASIN
    asin = "B08N5WRWNW"  # ç¤ºä¾‹å•†å“ASIN / Example product ASIN
    print(f"\næ­£åœ¨é‡‡é›†å•†å“è¯¦æƒ… / Scraping product detail: {asin}")
    
    detail = scraper.scrape_product_detail(asin)
    
    if detail:
        print("\nâœ… è¯¦æƒ…é‡‡é›†å®Œæˆ / Detail scraping completed")
        print(f"\nå•†å“è¯¦æƒ… / Product Detail:")
        print(f"  æ ‡é¢˜ / Title: {detail.get('title', 'N/A')[:50]}...")
        print(f"  ä»·æ ¼ / Price: {detail.get('price', 'N/A')}")
        print(f"  è¯„åˆ† / Rating: {detail.get('rating', 'N/A')}")
        print(f"  è¯„è®ºæ•° / Reviews: {detail.get('review_count', 'N/A')}")
        print(f"  å“ç‰Œ / Brand: {detail.get('brand', 'N/A')}")


def example3_save_data():
    """
    ç¤ºä¾‹3: æ•°æ®ä¿å­˜
    Example 3: Data saving
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹3: æ•°æ®ä¿å­˜ / Example 3: Data Saving")
    print("=" * 60)
    
    scraper = AmazonScraper()
    
    # æ¨¡æ‹Ÿé‡‡é›†æ•°æ® / Mock scraped data
    sample_data = [
        {
            "asin": "TEST001",
            "title": "Sample Product 1",
            "price": "$29.99",
            "rating": "4.5 out of 5 stars",
            "url": "https://amazon.com/dp/TEST001"
        },
        {
            "asin": "TEST002",
            "title": "Sample Product 2",
            "price": "$39.99",
            "rating": "4.8 out of 5 stars",
            "url": "https://amazon.com/dp/TEST002"
        }
    ]
    
    print(f"\nä¿å­˜ {len(sample_data)} ä¸ªå•†å“æ•°æ® / Saving {len(sample_data)} products")
    
    filepath = scraper.save_data(sample_data, filename="example_products.json")
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° / Data saved to: {filepath}")
    
    # è¯»å–å¹¶éªŒè¯ / Read and verify
    with open(filepath, 'r', encoding='utf-8') as f:
        saved_data = json.load(f)
    
    print(f"\néªŒè¯ / Verification:")
    print(f"  æ€»æ•°é‡ / Total count: {saved_data['total_count']}")
    print(f"  é‡‡é›†æ—¶é—´ / Scraped at: {saved_data['scraped_at']}")
    
    # æ¸…ç†ç¤ºä¾‹æ–‡ä»¶ / Cleanup example file
    os.remove(filepath)
    print(f"\nâœ… ç¤ºä¾‹æ–‡ä»¶å·²æ¸…ç† / Example file cleaned up")


def example4_convenience_function():
    """
    ç¤ºä¾‹4: ä½¿ç”¨ä¾¿æ·å‡½æ•°
    Example 4: Using convenience function
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹4: ä½¿ç”¨ä¾¿æ·å‡½æ•° / Example 4: Using Convenience Function")
    print("=" * 60)
    
    url = "https://www.amazon.com/bestsellers"
    print(f"\nä½¿ç”¨ä¾¿æ·å‡½æ•°é‡‡é›† / Using convenience function to scrape:")
    print(f"URL: {url}")
    
    # æ³¨æ„ï¼šè¿™ä¼šå®é™…è®¿é—®Amazonï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
    # Note: This will actually visit Amazon, may take some time
    # åœ¨æ¼”ç¤ºæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è·³è¿‡å®é™…æ‰§è¡Œ
    # In demo mode, we skip actual execution
    
    print("\nä»£ç ç¤ºä¾‹ / Code example:")
    print("""
    from scrapers.amazon_scraper import scrape_amazon
    
    # å¿«é€Ÿé‡‡é›† / Quick scraping
    products = scrape_amazon(
        url="https://www.amazon.com/bestsellers",
        max_items=50,
        deep_detail=False
    )
    
    # å¸¦è¯¦æƒ…é‡‡é›† / With detail scraping
    products_with_detail = scrape_amazon(
        url="https://www.amazon.com/s?k=headphones",
        max_items=20,
        deep_detail=True
    )
    """)
    
    print("\nğŸ’¡ æç¤º: å®é™…ä½¿ç”¨æ—¶å–æ¶ˆæ³¨é‡Šä¸Šé¢çš„ä»£ç ")
    print("ğŸ’¡ Tip: Uncomment the code above for actual usage")


def example5_batch_urls():
    """
    ç¤ºä¾‹5: æ‰¹é‡URLé‡‡é›†
    Example 5: Batch URL scraping
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹5: æ‰¹é‡URLé‡‡é›† / Example 5: Batch URL Scraping")
    print("=" * 60)
    
    urls = [
        "https://www.amazon.com/s?k=laptop",
        "https://www.amazon.com/s?k=headphones",
        "https://www.amazon.com/bestsellers"
    ]
    
    print(f"\nå‡†å¤‡é‡‡é›† {len(urls)} ä¸ªURL / Preparing to scrape {len(urls)} URLs:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    
    print("\nä»£ç ç¤ºä¾‹ / Code example:")
    print("""
    from core.crawl.dispatcher import run_batch
    
    # æ‰¹é‡é‡‡é›† / Batch scraping
    run_batch(urls, storage_mode="local")
    """)
    
    print("\nğŸ’¡ æç¤º: ä½¿ç”¨dispatcherå¯ä»¥è‡ªåŠ¨å¤„ç†å¤šä¸ªURL")
    print("ğŸ’¡ Tip: Use dispatcher to automatically handle multiple URLs")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    Main function: Run all examples
    """
    print("\n" + "=" * 60)
    print("Amazonçˆ¬è™«ä½¿ç”¨ç¤ºä¾‹é›†")
    print("Amazon Scraper Usage Examples")
    print("=" * 60)
    
    try:
        # è¿è¡ŒåŸºç¡€ç¤ºä¾‹ / Run basic examples
        example3_save_data()  # åªè¿è¡Œä¸éœ€è¦ç½‘ç»œè¯·æ±‚çš„ç¤ºä¾‹
        
        print("\n" + "=" * 60)
        print("å…¶ä»–ç¤ºä¾‹ / Other Examples")
        print("=" * 60)
        print("\nä»¥ä¸‹ç¤ºä¾‹éœ€è¦å®é™…ç½‘ç»œè¯·æ±‚ï¼Œæ¼”ç¤ºæ¨¡å¼ä¸‹è·³è¿‡:")
        print("The following examples require actual network requests, skipped in demo mode:")
        print("  - ç¤ºä¾‹1: åŸºç¡€é‡‡é›† / Example 1: Basic Scraping")
        print("  - ç¤ºä¾‹2: è¯¦æƒ…é¡µé‡‡é›† / Example 2: Detail Page Scraping")
        print("  - ç¤ºä¾‹4: ä¾¿æ·å‡½æ•° / Example 4: Convenience Function")
        print("  - ç¤ºä¾‹5: æ‰¹é‡é‡‡é›† / Example 5: Batch Scraping")
        
        print("\nğŸ’¡ æç¤º: ä¿®æ”¹è„šæœ¬ä»¥è¿è¡Œå…¶ä»–ç¤ºä¾‹")
        print("ğŸ’¡ Tip: Modify the script to run other examples")
        
        example4_convenience_function()
        example5_batch_urls()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ / User interrupted")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯ / Error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹æ¼”ç¤ºå®Œæˆ / All examples demonstrated")
    print("=" * 60)
    print("\næŸ¥çœ‹æ›´å¤šæ–‡æ¡£ / See more documentation:")
    print("  - README.md")
    print("  - scrapers/amazon_scraper.py")
    print("  - test/unit/test_amazon_scraper.py")
    print()


if __name__ == "__main__":
    main()
