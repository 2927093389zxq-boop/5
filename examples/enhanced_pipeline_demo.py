"""
Integration Example: All Future Enhancements Working Together
é›†æˆç¤ºä¾‹ï¼šæ‰€æœ‰æœªæ¥å¢å¼ºåŠŸèƒ½ååŒå·¥ä½œ

This example demonstrates how to use all four enhancements together:
1. Browser automation for JavaScript-rendered content
2. Task queue for distributed scraping
3. Data validation and deduplication
4. Real-time monitoring dashboard

æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åŒæ—¶ä½¿ç”¨æ‰€æœ‰å››é¡¹å¢å¼ºåŠŸèƒ½ï¼š
1. ç”¨äº JavaScript æ¸²æŸ“å†…å®¹çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–
2. ç”¨äºåˆ†å¸ƒå¼æŠ“å–çš„ä»»åŠ¡é˜Ÿåˆ—
3. æ•°æ®éªŒè¯å’Œå»é‡
4. å®æ—¶ç›‘æ§ä»ªè¡¨æ¿
"""

import asyncio
import time
from typing import Dict, Any, List

from core.browser_automation import BrowserAutomation
from core.task_queue import TaskQueue, Task
from core.data_validation import (
    DataQualityChecker,
    create_amazon_validator,
    create_amazon_deduplicator
)
from core.monitoring import get_monitoring_dashboard
from scrapers.amazon_scraper import AmazonScraper
from scrapers.logger import log_info, log_error


class EnhancedScrapingPipeline:
    """
    Enhanced scraping pipeline with all four improvements
    å…·æœ‰æ‰€æœ‰å››é¡¹æ”¹è¿›çš„å¢å¼ºå‹æŠ“å–ç®¡é“
    """
    
    def __init__(self, max_workers: int = 4, use_browser: bool = False):
        """
        Initialize pipeline
        åˆå§‹åŒ–ç®¡é“
        
        Args:
            max_workers: Number of worker threads / å·¥ä½œçº¿ç¨‹æ•°
            use_browser: Use browser automation / ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–
        """
        # 1. Setup monitoring
        self.dashboard = get_monitoring_dashboard()
        log_info("ç›‘æ§ä»ªè¡¨æ¿å·²åˆå§‹åŒ– / Monitoring dashboard initialized")
        
        # 2. Setup data validation
        self.validator = create_amazon_validator()
        self.deduplicator = create_amazon_deduplicator()
        self.quality_checker = DataQualityChecker(self.validator, self.deduplicator)
        log_info("æ•°æ®éªŒè¯å™¨å·²åˆå§‹åŒ– / Data validator initialized")
        
        # 3. Setup task queue
        self.queue = TaskQueue(max_workers=max_workers)
        self.queue.register_handler("scrape_url", self._scrape_handler)
        log_info(f"ä»»åŠ¡é˜Ÿåˆ—å·²åˆå§‹åŒ–ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {max_workers} / Task queue initialized with {max_workers} workers")
        
        # 4. Setup browser automation (optional)
        self.use_browser = use_browser
        self.browser = None
        if use_browser:
            self.browser = BrowserAutomation(headless=True)
            log_info("æµè§ˆå™¨è‡ªåŠ¨åŒ–å·²å¯ç”¨ / Browser automation enabled")
    
    def _scrape_handler(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Task handler for scraping
        æŠ“å–ä»»åŠ¡å¤„ç†å™¨
        
        Args:
            params: Task parameters / ä»»åŠ¡å‚æ•°
            
        Returns:
            Task result / ä»»åŠ¡ç»“æœ
        """
        url = params["url"]
        platform = params.get("platform", "amazon")
        max_items = params.get("max_items", 50)
        
        log_info(f"å¼€å§‹æŠ“å– / Starting scrape: {url}")
        start_time = time.time()
        
        try:
            # Use browser automation for JS-heavy sites if enabled
            if self.use_browser and self.browser:
                log_info(f"ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ– / Using browser automation: {url}")
                # Note: This is async, would need to be called properly in async context
                # For now, fall back to traditional scraper
                # In production, you'd use: content = await self.browser.get_page_content(url)
                scraper = AmazonScraper()
                items = scraper.scrape_list_page(url, max_items=max_items)
            else:
                # Use traditional scraper for static sites
                scraper = AmazonScraper()
                items = scraper.scrape_list_page(url, max_items=max_items)
            
            if not items:
                log_error(f"æœªæŠ“å–åˆ°æ•°æ® / No data scraped: {url}")
                response_time = time.time() - start_time
                self.dashboard.record_scraping_operation(
                    platform=platform,
                    success=False,
                    response_time=response_time,
                    error_type="no_data"
                )
                return {"success": False, "error": "No data scraped"}
            
            # Validate and deduplicate
            log_info(f"éªŒè¯å’Œå»é‡ {len(items)} ä¸ªé¡¹ç›® / Validating and deduplicating {len(items)} items")
            quality_report = self.quality_checker.check(items)
            clean_data = quality_report["valid_data"]
            
            log_info(f"è´¨é‡æ£€æŸ¥å®Œæˆ / Quality check complete: "
                    f"æœ‰æ•ˆ={quality_report['valid_count']}, "
                    f"æ— æ•ˆ={quality_report['invalid_count']}, "
                    f"å”¯ä¸€={quality_report['unique_count']}, "
                    f"é‡å¤={quality_report['duplicate_count']}")
            
            # Record success in monitoring
            response_time = time.time() - start_time
            self.dashboard.record_scraping_operation(
                platform=platform,
                success=True,
                response_time=response_time,
                items_count=len(clean_data)
            )
            
            log_info(f"æŠ“å–æˆåŠŸ / Scrape successful: {url} ({len(clean_data)} items)")
            
            return {
                "success": True,
                "items": clean_data,
                "quality": {
                    "valid_count": quality_report["valid_count"],
                    "invalid_count": quality_report["invalid_count"],
                    "unique_count": quality_report["unique_count"],
                    "duplicate_count": quality_report["duplicate_count"],
                    "quality_score": quality_report["quality_score"]
                }
            }
            
        except Exception as e:
            log_error(f"æŠ“å–å¤±è´¥ / Scrape failed: {url} - {e}")
            
            # Record failure in monitoring
            response_time = time.time() - start_time
            self.dashboard.record_scraping_operation(
                platform=platform,
                success=False,
                response_time=response_time,
                error_type=type(e).__name__
            )
            
            return {"success": False, "error": str(e)}
    
    def add_tasks(self, urls: List[str], platform: str = "amazon", max_items: int = 50):
        """
        Add scraping tasks to queue
        æ·»åŠ æŠ“å–ä»»åŠ¡åˆ°é˜Ÿåˆ—
        
        Args:
            urls: List of URLs to scrape / è¦æŠ“å–çš„ URL åˆ—è¡¨
            platform: Platform name / å¹³å°åç§°
            max_items: Maximum items per URL / æ¯ä¸ª URL çš„æœ€å¤§é¡¹ç›®æ•°
        """
        log_info(f"æ·»åŠ  {len(urls)} ä¸ªä»»åŠ¡åˆ°é˜Ÿåˆ— / Adding {len(urls)} tasks to queue")
        
        for i, url in enumerate(urls):
            task = Task(
                task_id=f"task_{i}",
                task_type="scrape_url",
                params={
                    "url": url,
                    "platform": platform,
                    "max_items": max_items
                },
                priority=i  # Higher priority for first tasks
            )
            self.queue.add_task(task)
    
    async def run(self):
        """
        Run the scraping pipeline
        è¿è¡ŒæŠ“å–ç®¡é“
        """
        log_info("å¯åŠ¨æŠ“å–ç®¡é“ / Starting scraping pipeline")
        
        # Start queue
        self.queue.start()
        
        # Wait for all tasks to complete
        while True:
            stats = self.queue.get_stats()
            
            if stats["running"] == 0 and stats["pending"] == 0:
                break
            
            # Log progress
            log_info(f"é˜Ÿåˆ—çŠ¶æ€ / Queue status: "
                    f"å¾…å¤„ç†={stats['pending']}, "
                    f"è¿è¡Œä¸­={stats['running']}, "
                    f"å·²å®Œæˆ={stats['completed']}, "
                    f"å¤±è´¥={stats['failed']}")
            
            await asyncio.sleep(2)
        
        # Stop queue
        self.queue.stop()
        
        # Get final statistics
        final_stats = self.queue.get_stats()
        dashboard_data = self.dashboard.get_dashboard_data()
        current_stats = dashboard_data["current_stats"]
        
        log_info("=" * 60)
        log_info("æŠ“å–ç®¡é“å®Œæˆ / Scraping pipeline completed")
        log_info("=" * 60)
        log_info(f"ä»»åŠ¡ç»Ÿè®¡ / Task Statistics:")
        log_info(f"  æ€»ä»»åŠ¡ / Total Tasks: {final_stats['total']}")
        log_info(f"  å·²å®Œæˆ / Completed: {final_stats['completed']}")
        log_info(f"  å¤±è´¥ / Failed: {final_stats['failed']}")
        log_info(f"  å–æ¶ˆ / Cancelled: {final_stats['cancelled']}")
        log_info("")
        log_info(f"ç›‘æ§ç»Ÿè®¡ / Monitoring Statistics:")
        log_info(f"  æ€»è¯·æ±‚ / Total Requests: {current_stats['total_requests']}")
        log_info(f"  æˆåŠŸç‡ / Success Rate: {current_stats['success_rate']:.1f}%")
        log_info(f"  æŠ“å–é¡¹ç›® / Items Scraped: {current_stats['total_items_scraped']}")
        log_info(f"  å¹³å‡å“åº”æ—¶é—´ / Avg Response Time: {current_stats['avg_response_time']:.2f}s")
        log_info(f"  é”™è¯¯æ•° / Errors: {current_stats['total_errors']}")
        log_info("=" * 60)
        
        # Cleanup
        if self.browser:
            await self.browser.close()
    
    def print_monitoring_dashboard(self):
        """Print monitoring dashboard summary / æ‰“å°ç›‘æ§ä»ªè¡¨æ¿æ‘˜è¦"""
        data = self.dashboard.get_dashboard_data()
        
        print("\n" + "=" * 80)
        print("MONITORING DASHBOARD / ç›‘æ§ä»ªè¡¨æ¿")
        print("=" * 80)
        
        # Current stats
        stats = data["current_stats"]
        print(f"\nğŸ“Š Overall Statistics / æ€»ä½“ç»Ÿè®¡:")
        print(f"  Total Requests / æ€»è¯·æ±‚: {stats['total_requests']}")
        print(f"  Success Rate / æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"  Items Scraped / æŠ“å–é¡¹ç›®: {stats['total_items_scraped']}")
        print(f"  Errors / é”™è¯¯: {stats['total_errors']}")
        print(f"  Captcha Hits / éªŒè¯ç : {stats['captcha_hits']}")
        
        # Performance
        print(f"\nâš¡ Performance / æ€§èƒ½:")
        print(f"  Avg Response Time / å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}s")
        print(f"  Min Response Time / æœ€å°å“åº”æ—¶é—´: {stats['min_response_time']:.2f}s")
        print(f"  Max Response Time / æœ€å¤§å“åº”æ—¶é—´: {stats['max_response_time']:.2f}s")
        print(f"  Requests/min / è¯·æ±‚/åˆ†é’Ÿ: {stats['requests_per_minute']:.1f}")
        
        # Platform stats
        if data["platform_stats"]:
            print(f"\nğŸŒ Platform Statistics / å¹³å°ç»Ÿè®¡:")
            for platform, pstats in data["platform_stats"].items():
                success_rate = (pstats["successful"] / pstats["requests"] * 100 
                              if pstats["requests"] > 0 else 0)
                print(f"  {platform}:")
                print(f"    Requests / è¯·æ±‚: {pstats['requests']}")
                print(f"    Success Rate / æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"    Items / é¡¹ç›®: {pstats['items']}")
        
        # Alerts
        if data["alerts"]:
            print(f"\nğŸš¨ Recent Alerts / æœ€è¿‘è­¦æŠ¥:")
            for alert in data["alerts"][-5:]:  # Last 5 alerts
                severity_icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(alert["severity"], "âšª")
                print(f"  {severity_icon} [{alert['timestamp'][:19]}] {alert['message']}")
        
        print("=" * 80 + "\n")


async def main():
    """Main function / ä¸»å‡½æ•°"""
    
    print("\n" + "=" * 80)
    print("ENHANCED SCRAPING PIPELINE DEMO / å¢å¼ºå‹æŠ“å–ç®¡é“æ¼”ç¤º")
    print("=" * 80)
    print("\nThis demo showcases all four enhancements:")
    print("æ­¤æ¼”ç¤ºå±•ç¤ºæ‰€æœ‰å››é¡¹å¢å¼ºåŠŸèƒ½ï¼š")
    print("  1. Browser Automation (Playwright) / æµè§ˆå™¨è‡ªåŠ¨åŒ–")
    print("  2. Distributed Task Queue / åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—")
    print("  3. Data Validation & Deduplication / æ•°æ®éªŒè¯å’Œå»é‡")
    print("  4. Real-time Monitoring / å®æ—¶ç›‘æ§")
    print("=" * 80 + "\n")
    
    # Create pipeline
    pipeline = EnhancedScrapingPipeline(
        max_workers=2,  # Use 2 workers for demo
        use_browser=False  # Set to True to enable browser automation
    )
    
    # Example URLs (modify as needed for your use case)
    # ç¤ºä¾‹ URLï¼ˆæ ¹æ®æ‚¨çš„ç”¨ä¾‹éœ€è¦è¿›è¡Œä¿®æ”¹ï¼‰
    urls = [
        "https://www.amazon.com/bestsellers",
        "https://www.amazon.com/s?k=laptop",
        # Add more URLs as needed / æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤š URL
    ]
    
    # Alternative: Use environment variable or config file for URLs
    # æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶å­˜å‚¨ URL
    # import os
    # urls = os.getenv("SCRAPE_URLS", "").split(",")
    
    print(f"å‡†å¤‡æŠ“å– {len(urls)} ä¸ª URL / Preparing to scrape {len(urls)} URLs")
    
    # Add tasks
    pipeline.add_tasks(urls, platform="amazon", max_items=20)
    
    # Run pipeline
    await pipeline.run()
    
    # Print final dashboard
    pipeline.print_monitoring_dashboard()
    
    print("\næ¼”ç¤ºå®Œæˆï¼ / Demo completed!")
    print("æŸ¥çœ‹ Streamlit ä»ªè¡¨æ¿ä»¥è·å–å®æ—¶ç›‘æ§ / Check Streamlit dashboard for real-time monitoring")
    print("è¿è¡Œ: python -m streamlit run ui/monitoring_view.py")
    print("æˆ–: python run_launcher.py  (å¦‚æœä½¿ç”¨ä¸»å¯åŠ¨å™¨)")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    # Note: In production, you might want to run this with proper error handling
    # æ³¨æ„ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨é€‚å½“çš„é”™è¯¯å¤„ç†è¿è¡Œæ­¤ç¨‹åº
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nä¸­æ–­ï¼šç”¨æˆ·å–æ¶ˆ / Interrupted: User cancelled")
    except Exception as e:
        print(f"\n\né”™è¯¯ / Error: {e}")
        import traceback
        traceback.print_exc()
